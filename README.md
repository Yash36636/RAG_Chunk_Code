# Lenny's Podcast Second Brain

> A product-management-focused conversational RAG system that synthesizes insights from 100+ hours of Lenny's Podcast conversations with top product leaders.

![Version](https://img.shields.io/badge/version-4.0.0-blue)
![Python](https://img.shields.io/badge/python-3.9+-green)
![License](https://img.shields.io/badge/license-MIT-gray)

**üîó Live Demo:** [Coming Soon]

---

## üéØ What This Is

**Lenny's Podcast Second Brain** is an AI-powered knowledge system that:

- Answers product management questions using **real expert conversations**
- Shows **confidence levels** based on source quality (not guessing)
- Provides **clickable citations** to original YouTube timestamps
- Generates **depth-building follow-ups** like a senior PM mentor
- Gracefully handles off-topic or unsafe queries

**This is NOT a general chatbot.** It's a grounded knowledge system that only speaks with authority when it has strong evidence.

---

## üß† Key Design Decisions

### 1. Confidence Gating (Most Important)

**Problem:** LLMs confidently hallucinate when they don't know.

**Solution:** We compute confidence from FAISS retrieval scores *before* synthesis. Low confidence = we change behavior.

| Confidence | Behavior |
|------------|----------|
| **High** (‚â•0.65) | Authoritative answer with citations |
| **Medium** (‚â•0.52) | Balanced insights, acknowledge limitations |
| **Low** (<0.52) | Mentor-style conversation, no false authority |

### 2. Depth-Building Follow-ups

**Problem:** Generic follow-ups like "Tell me more?" feel like a support chatbot.

**Solution:** Follow-ups are generated from:
- The answer just given
- Themes from retrieved sources
- A "senior PM mentor" prompt

Result: Questions that push deeper thinking, not generic probing.

### 3. Source Diversity Constraint

**Problem:** Top results might all be from the same video, creating echo-chamber answers.

**Solution:** Max one citation per video. Forces diversity in perspectives.

### 4. Frontend Never Fabricates

**Problem:** Client-side fallbacks can show fake citations.

**Solution:** Frontend renders ONLY what backend returns. Zero client-side generation.

---

## üèóÔ∏è Architecture

```
User Query
    ‚îÇ
    ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Safety Check   ‚îÇ ‚îÄ‚îÄ‚Üí Block harmful queries
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FAISS Retrieval ‚îÇ ‚îÄ‚îÄ‚Üí Find similar chunks (score ‚â• 0.60)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Confidence Gate ‚îÇ ‚îÄ‚îÄ‚Üí HIGH/MEDIUM ‚Üí RAG Mode
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      LOW ‚Üí Conversation Mode
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ LLM Synthesis   ‚îÇ ‚îÄ‚îÄ‚Üí Groq (fast) / Ollama (local)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Follow-up Gen   ‚îÇ ‚îÄ‚îÄ‚Üí Only if confidence ‚â• MEDIUM
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Strict Response ‚îÇ ‚îÄ‚îÄ‚Üí { answer, sources, confidence, followups }
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üöÄ Quick Start (Local Development)

### Prerequisites

- Python 3.9+
- Node.js 18+
- [Groq API key](https://console.groq.com) (free)

### Backend

```bash
cd RAG_Chunk_Code

# Install dependencies
pip install -r requirements.txt

# Set Groq API key
export GROQ_API_KEY=your_key_here  # Mac/Linux
$env:GROQ_API_KEY="your_key_here"  # Windows PowerShell

# Start server
python -m uvicorn server:app --host 0.0.0.0 --port 8000
```

### Frontend

```bash
cd frontend
npm install
npm run dev
```

Visit **http://localhost:3000**

---

## ‚òÅÔ∏è Hosting (Production)

### Backend ‚Üí Render

1. Push repo to GitHub
2. Go to [render.com](https://render.com) ‚Üí New Web Service
3. Connect your GitHub repo
4. Configure:
   - **Build command:** `pip install -r requirements.txt`
   - **Start command:** `uvicorn server:app --host 0.0.0.0 --port $PORT`
   - **Environment variables:** `GROQ_API_KEY=your_key`, `LLM_PROVIDER=groq`

Or use the included `render.yaml` for one-click deploy.

### Frontend ‚Üí Vercel

1. Go to [vercel.com](https://vercel.com) ‚Üí New Project
2. Import your GitHub repo
3. Set root directory to `frontend/`
4. Add environment variable:
   - `NEXT_PUBLIC_API_URL=https://your-backend.onrender.com`
5. Deploy

---

## üìä API Reference

### POST /query

```json
// Request
{
  "query": "What makes a great product manager?",
  "session_id": "optional-uuid"
}

// Response
{
  "answer": {
    "direct_answer": "...",
    "key_ideas": ["...", "..."],
    "common_pitfall": "...",
    "summary": "..."
  },
  "sources": [
    {
      "video_title": "Episode with Shreyas Doshi",
      "speaker": "Shreyas Doshi",
      "thumbnail": "https://img.youtube.com/vi/xxx/mqdefault.jpg",
      "timestamp": "14:32",
      "link": "https://youtube.com/watch?v=xxx&t=872",
      "score": 0.72
    }
  ],
  "confidence": "high",
  "mode": "rag",
  "followups": [
    "Which of these traits is hardest to develop early in a PM career?",
    "How would you demonstrate accountability when you don't own execution?"
  ],
  "latency_ms": 1200
}
```

---

## üìÅ Project Structure

```
RAG_Chunk_Code/
‚îú‚îÄ‚îÄ server.py                 # FastAPI entry point
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ confidence.py         # Confidence scoring (‚â•0.60 threshold)
‚îÇ   ‚îú‚îÄ‚îÄ safety.py             # Safety guards
‚îÇ   ‚îú‚îÄ‚îÄ followup_generator.py # Depth-building follow-ups
‚îÇ   ‚îú‚îÄ‚îÄ unified_synthesizer.py
‚îÇ   ‚îú‚îÄ‚îÄ retrieval.py          # FAISS search
‚îÇ   ‚îú‚îÄ‚îÄ memory.py             # Session memory
‚îÇ   ‚îú‚îÄ‚îÄ prompts/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ cached_system.py  # Static prompts (Groq caching)
‚îÇ   ‚îî‚îÄ‚îÄ llm/
‚îÇ       ‚îú‚îÄ‚îÄ groq_llm.py       # Groq client
‚îÇ       ‚îî‚îÄ‚îÄ ollama_llm.py     # Ollama client
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ app/page.tsx          # Main chat UI
‚îÇ   ‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ AnswerPanel.tsx   # Structured answer display
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ CitationSidebar.tsx
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ data/mockData.ts      # API client
‚îú‚îÄ‚îÄ faiss_indexes/            # Vector embeddings
‚îú‚îÄ‚îÄ chunks_product_management/ # Source transcripts
‚îú‚îÄ‚îÄ render.yaml               # Render deployment config
‚îî‚îÄ‚îÄ requirements.txt
```

---

## ‚ö†Ô∏è Limitations (Documented Intentionally)

| Limitation | Reason | Trade-off |
|------------|--------|-----------|
| Session memory is ephemeral | Avoids database complexity | Memory resets on server restart |
| No authentication | Demo project scope | Anyone can query |
| English only | Source transcripts are English | Could add multilingual later |
| ~1-2s latency | Groq API round-trip | Acceptable for demo |

These are **intentional scope decisions**, not bugs.

---

## üß† Why RAG vs Fine-Tuning?

| Approach | RAG (This Project) | Fine-Tuning |
|----------|-------------------|-------------|
| **Data freshness** | ‚úÖ Update instantly | ‚ùå Requires retraining |
| **Transparency** | ‚úÖ Shows exact sources | ‚ùå Black box |
| **Hallucination control** | ‚úÖ Grounded in retrieval | ‚ö†Ô∏è Can invent facts |
| **Cost** | ‚úÖ Low (API calls) | ‚ùå High (GPU training) |
| **Auditability** | ‚úÖ Can verify every claim | ‚ùå Hard to trace |

**Key insight:** For knowledge-intensive applications where trust matters, RAG provides accountability that fine-tuning cannot.

---

## üé§ How to Explain This (2-Minute Version)

> "I built a production-grade RAG system that answers product management questions using Lenny's Podcast transcripts.
>
> The key insight is that LLMs confidently hallucinate when they don't know. So I implemented **confidence gating** ‚Äî we score retrieval quality BEFORE synthesis. High confidence gets authoritative answers with citations. Low confidence switches to a mentor-style conversation, never false authority.
>
> Follow-ups aren't generic 'tell me more' questions ‚Äî they're generated from the answer and sources to push deeper PM thinking, like a senior mentor would.
>
> I hosted the backend and frontend separately so they can scale independently. Session memory is intentionally ephemeral to avoid premature database complexity ‚Äî I documented that trade-off.
>
> The frontend respects a strict contract ‚Äî it renders ONLY what the backend returns. No client-side generation means no fake citations.
>
> This demonstrates senior-level judgment: knowing when to trust the system, when to be humble, and when to document trade-offs instead of over-engineering."

---

## üîÆ Future Improvements

- [ ] Streaming responses for better UX
- [ ] Persistent conversation memory (Redis/Supabase)
- [ ] User feedback loop for retrieval quality
- [ ] Admin panel for content management
- [ ] Multi-language support

---

## üìù License

MIT

---

Built with ‚ù§Ô∏è by a PM who believes in grounded, trustworthy AI systems.
